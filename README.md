# Naive Bayes Classifier (nbc)

なんとなく気分で書いた nbc (単純ベイズ識別器)。  
正確には識別ではなくクラスタリングを行うという罠。  

# 使い方

nbc.rb は csv ファイルを入れればとりあえず動きます。   
実数値は正規分布で、離散値はカテゴリカル分布(サイコロの分布)で扱います。   

とりあえず [UCI repository][] の [iris][] データをクラスタリングしてみます。 
[iris.data][] をダウンロードし、以下のように入力してください。

    ./nbc.rb -f iris.data -y 4

するとなんやかんやでクラスタリングできます。  
オプションの -y 4 というのは、4番目の属性は答えなのでクラスタリングに考慮するなという意味です。   
オプション -I a,b,c によって a,b,c 番目の属性を無視することもできます。  
ただ、-y で答えを設定しておくとクラスタリング結果を見るときに少し見やすくなります。

デフォルトでは３クラスタに分けるます。   
クラスタ数を自分で指定する場合は、以下のように -k option で指定してください。

    ./nbc.rb -f iris.data -y 4 -k 5

上だと５クラスタに分割します。
他にもいくつかオプションはありますが、詳しくは以下で見てください。

    ./nbc.rb -h


[UCI repository]: http://archive.ics.uci.edu/ml/
[iris]: http://archive.ics.uci.edu/ml/datasets/Iris
[iris.data]: http://archive.ics.uci.edu/ml/machine-learning-databases/iris/iris.data



# クラスタリング手法

nbc.rb では naive Bayes (NB) という確率モデルのパラメータを Viterbi
Training (VT) という方法により学習を行い、クラスタリングを実現していま
す。  

NB は各データはクラスタが与えられたとき、すべての属性は独立に生成されることを仮定した確率モデルです。  
クラスタは未観測なのでそれを推定するわけですが、推定するには確率値が必要となります。  
確率モデルのパラメータを推定する方法はたくさんありますが、今回は VT を用いました。
VT ではまず確率値を適当に初期化し、未観測部分を viterbi 値で埋めます。
viterbi 値とはもっとも確率が高い（尤もらしい）値のことです。
これにより未観測変数はなくなるので、確率値をカウンティングに基づいて更新します。   
この操作を未観測部分の割り当てが変わらなくなるまで続けます。   

未観測変数を含む場合の有名な学習法に EM アルゴリズムがあります。  
EM アルゴリズムは確率値の更新に必要なカウントを現在の確率値における期待値で置きかえるテクニックです。   
一般に VT は EM よりも実装が楽、高速である、という良い性質を持つ反面、
過学習しやすい、初期値に依存されやすいという特徴もあります。  
VT, EM はともに局所探索なので、初期値を変えて再学習を何度か行い、もっとも尤度（尤もらしさ）が高かった結果を出力するのが通例です。   
nbc.rb では -i オプションで再学習の回数を指定できます。